{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision # some builtin datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to one architechure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "\n",
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# hyper parameters\n",
    "\n",
    "# input_size = 28*28 \n",
    "# we will treat the image as a sequence\n",
    "# look at one row at a time\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "\n",
    "num_classes = 10\n",
    "num_epoches = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenya/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# MNIST\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", \n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),#Converts the images into PyTorch tensors. Each image is normalized to have values in the range [0,1]\n",
    "    download=True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", \n",
    "    train=False,\n",
    "    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "# [100, 1, 28, 28]: batch size 100, gray image with size 28 x28\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # batch_first: If True, then the input and output tensors are provided as (batch, seq, input_size)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # batch_size, sequence_length, hidden size\n",
    "        out = out[:, -1, :] #only the last step\n",
    "        # batch_size, hidden size\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step 100 / 600, loss =  0.8658\n",
      "epoch 1 / 2, step 200 / 600, loss =  0.4443\n",
      "epoch 1 / 2, step 300 / 600, loss =  0.2639\n",
      "epoch 1 / 2, step 400 / 600, loss =  0.2302\n",
      "epoch 1 / 2, step 500 / 600, loss =  0.2061\n",
      "epoch 1 / 2, step 600 / 600, loss =  0.1723\n",
      "epoch 2 / 2, step 100 / 600, loss =  0.1643\n",
      "epoch 2 / 2, step 200 / 600, loss =  0.0392\n",
      "epoch 2 / 2, step 300 / 600, loss =  0.0602\n",
      "epoch 2 / 2, step 400 / 600, loss =  0.1558\n",
      "epoch 2 / 2, step 500 / 600, loss =  0.0681\n",
      "epoch 2 / 2, step 600 / 600, loss =  0.1165\n",
      "acc = 97.4800\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(images.shape) # torch.Size([100, 1, 28, 28]) --> 100, 784\n",
    "        # print(labels.shape) # torch.Size([100])\n",
    "        \n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epoches}, step {i+1} / {num_total_steps}, loss = {loss: 0.4f}')\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_eva = model(images)\n",
    "        _, y_eva_cls = torch.max(y_eva, 1)\n",
    "        \n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (y_eva_cls == labels).sum().item()\n",
    "    acc = 100*n_correct/n_samples\n",
    "    print(f'acc = {acc:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

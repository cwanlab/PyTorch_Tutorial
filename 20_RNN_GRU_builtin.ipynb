{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision # some builtin datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to one architechure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "\n",
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# hyper parameters\n",
    "\n",
    "# input_size = 28*28 \n",
    "# we will treat the image as a sequence\n",
    "# look at one row at a time\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "\n",
    "num_classes = 10\n",
    "num_epoches = 2\n",
    "batch_size = 100\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", \n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),#Converts the images into PyTorch tensors. Each image is normalized to have values in the range [0,1]\n",
    "    download=True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", \n",
    "    train=False,\n",
    "    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "# [100, 1, 28, 28]: batch size 100, gray image with size 28 x28\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # batch_first: If True, then the input and output tensors are provided as (batch, seq, input_size)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # batch_size, sequence_length, hidden size\n",
    "        out = out[:, -1, :] #only the last step\n",
    "        # batch_size, hidden size\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step 100 / 600, loss =  0.7424\n",
      "epoch 1 / 2, step 200 / 600, loss =  0.3401\n",
      "epoch 1 / 2, step 300 / 600, loss =  0.3422\n",
      "epoch 1 / 2, step 400 / 600, loss =  0.1661\n",
      "epoch 1 / 2, step 500 / 600, loss =  0.1502\n",
      "epoch 1 / 2, step 600 / 600, loss =  0.0945\n",
      "epoch 2 / 2, step 100 / 600, loss =  0.0982\n",
      "epoch 2 / 2, step 200 / 600, loss =  0.0236\n",
      "epoch 2 / 2, step 300 / 600, loss =  0.0527\n",
      "epoch 2 / 2, step 400 / 600, loss =  0.1020\n",
      "epoch 2 / 2, step 500 / 600, loss =  0.0421\n",
      "epoch 2 / 2, step 600 / 600, loss =  0.1155\n",
      "acc = 97.4700\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(images.shape) # torch.Size([100, 1, 28, 28]) --> 100, 784\n",
    "        # print(labels.shape) # torch.Size([100])\n",
    "        \n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epoches}, step {i+1} / {num_total_steps}, loss = {loss: 0.4f}')\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_eva = model(images)\n",
    "        _, y_eva_cls = torch.max(y_eva, 1)\n",
    "        \n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (y_eva_cls == labels).sum().item()\n",
    "    acc = 100*n_correct/n_samples\n",
    "    print(f'acc = {acc:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

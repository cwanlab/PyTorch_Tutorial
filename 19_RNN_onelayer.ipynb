{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision # some builtin datasets\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import random\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n2 inputs: \\n# input tensor: a single item in the sequence\\n# hidden state tensor: initially zero\\n\\nA RNN layer has 3 weight matrices\\n# Input Dense\\n# Hidden Dense\\n# Output Dense\\n\\n2 outputs:\\n# new hidden state: activation(input dense * input + hidden dense * hidden state)\\n# output: activation(output dense * new hidden state)\\n\\nTask: predict next character\\n'"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "2 inputs: \n",
    "# input tensor: a single item in the sequence\n",
    "# hidden state tensor: initially zero\n",
    "\n",
    "A RNN layer has 3 weight matrices\n",
    "# Input Dense\n",
    "# Hidden Dense\n",
    "# Output Dense\n",
    "\n",
    "2 outputs:\n",
    "# new hidden state: activation(input dense * input + hidden dense * hidden state)\n",
    "# output: activation(output dense * new hidden state)\n",
    "\n",
    "Task: predict next character\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Data preparation\n",
    "\"\"\"\n",
    "class TextDataset(Dataset):\n",
    "    \"\"\"\n",
    "    Text Dataset Class\n",
    "    \n",
    "    This class is in charge of managing text data as vectors\n",
    "    Data is saved as vectors (not as text)\n",
    "\n",
    "    Attributes\n",
    "    ----------\n",
    "    seq_length - int: Sequence length\n",
    "    chars - list(str): List of characters\n",
    "    char_to_idx - dict: dictionary from character to index\n",
    "    idx_to_char - dict: dictionary from index to character\n",
    "    vocab_size - int: Vocabulary size\n",
    "    data_size - int: total length of the text\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, text_data: str, seq_length: int = 25) -> None:\n",
    "        \"\"\"\n",
    "        Inputs\n",
    "        ------\n",
    "        text_data: Full text data as string\n",
    "        seq_length: sequence length. How many characters per index of the dataset.\n",
    "        \"\"\"\n",
    "        self.chars = sorted(list(set(text_data)))\n",
    "        self.data_size, self.vocab_size = len(text_data), len(self.chars)\n",
    "        \n",
    "        # useful way to fetch characters either by index or char\n",
    "        self.idx_to_char = {i:ch for i, ch in enumerate(self.chars)}\n",
    "        self.char_to_idx = {ch:i for i, ch in enumerate(self.chars)}\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.X = self.string_to_vector(text_data)\n",
    "    \n",
    "    @property\n",
    "    def X_string(self) -> str:\n",
    "        \"\"\"\n",
    "        Returns X in string form\n",
    "        \"\"\"\n",
    "        return self.vector_to_string(self.X)\n",
    "        \n",
    "    def __len__(self) -> int:\n",
    "        \"\"\"\n",
    "        number of sequences\n",
    "\n",
    "        We remove the last sequence to avoid conflicts with Y being shifted to the left\n",
    "        This causes our model to never see the last sequence of text\n",
    "        which is not a huge deal, but its something to be aware of\n",
    "        \"\"\"\n",
    "        return int(len(self.X) / self.seq_length - 1)\n",
    "\n",
    "    def __getitem__(self, index) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        X and Y have the same shape, but Y is shifted left 1 position\n",
    "        \"\"\"\n",
    "        start_idx = index * self.seq_length\n",
    "        end_idx = (index + 1) * self.seq_length\n",
    "\n",
    "        X = torch.tensor(self.X[start_idx:end_idx]).float()\n",
    "        y = torch.tensor(self.X[start_idx+1:end_idx+1]).float()\n",
    "        return X, y\n",
    "    \n",
    "    def string_to_vector(self, name: str) -> list[int]:\n",
    "        \"\"\"\n",
    "        Converts a string into a 1D vector with values from char_to_idx dictionary\n",
    "\n",
    "        Inputs\n",
    "        name: Name as string\n",
    "\n",
    "        Outputs\n",
    "        name_tensor: name represented as list of integers (1D vector)\n",
    "\n",
    "        sample:\n",
    "        >>> string_to_vector('test')\n",
    "        [20, 5, 19, 20]\n",
    "        \"\"\"\n",
    "        vector = list()\n",
    "        for s in name:\n",
    "            vector.append(self.char_to_idx[s])\n",
    "        return vector\n",
    "\n",
    "    def vector_to_string(self, vector: list[int]) -> str:\n",
    "        \"\"\"\n",
    "        Converts a 1D vector into a string with values from idx_to_char dictionary\n",
    "\n",
    "        Inputs\n",
    "        vector: 1D vector with values in the range of idx_to_char\n",
    "\n",
    "        Outputs\n",
    "        vector_string: Vector converted to string\n",
    "\n",
    "        sample:\n",
    "        >>> vector_to_string([20, 5, 19, 20])\n",
    "        'test'\n",
    "        \"\"\"\n",
    "        vector_string = \"\"\n",
    "        for i in vector:\n",
    "            vector_string += self.idx_to_char[i]\n",
    "        return vector_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# test TextDataset\n",
    "if False:\n",
    "    # use any text file you want to learn\n",
    "    data = open('data/dinos.txt', 'r').read()\n",
    "    data = data.lower()\n",
    "\n",
    "    # Data size variables\n",
    "    seq_length = 25\n",
    "    batch_size = 3\n",
    "\n",
    "    text_dataset = TextDataset(data, seq_length=seq_length)\n",
    "    text_dataloader = DataLoader(text_dataset, batch_size)\n",
    "\n",
    "    # test\n",
    "    print(\"length:\", len(data))\n",
    "    print(\"unique characters:\", text_dataset.chars)\n",
    "\n",
    "    print(\"num_of_sequences:\", len(text_dataset))\n",
    "\n",
    "    examples = iter(text_dataloader)\n",
    "    X, y = examples.next()\n",
    "    print(X.shape, y.shape)\n",
    "    print(X[0,:])\n",
    "    print(y[0,:])\n",
    "    #print(text_dataset.X_string)\n",
    "    print(text_dataset.vector_to_string(X[0,:].tolist()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class RNN(nn.Module):\n",
    "    '''\n",
    "    Basic RNN block: a single layer of RNN.\n",
    "    '''\n",
    "    def __init__(self, input_size: int, hidden_size: int, output_size: int) -> None:\n",
    "        super().__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "        self.hidden_size = hidden_size\n",
    "        self.output_size = output_size\n",
    "\n",
    "        self.i2h = nn.Linear(input_size, hidden_size, bias=False)\n",
    "        self.h2h = nn.Linear(hidden_size, hidden_size)\n",
    "        self.h2o = nn.Linear(hidden_size, output_size)\n",
    "\n",
    "    def forward(self, x, h) -> tuple[torch.Tensor, torch.Tensor]:\n",
    "        x = self.i2h(x)\n",
    "        h = self.h2h(h)\n",
    "        h = torch.tanh(x + h)\n",
    "        \n",
    "        out = self.h2o(h)\n",
    "        return out, h\n",
    "    \n",
    "    def init_zero_hidden(self, batch_size=1) -> torch.Tensor:\n",
    "        return torch.zeros(batch_size, self.hidden_size, requires_grad=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model: RNN, data: DataLoader, epochs: int, optimizer: optim.Optimizer, loss_fn: nn.Module) -> None:\n",
    "    \"\"\"\n",
    "    Trains the model for the specified number of epochs\n",
    "\n",
    "    Inputs\n",
    "    ------\n",
    "    model: RNN model to train\n",
    "    data: Iterable DataLoader\n",
    "    epochs: Number of epochs to train the model\n",
    "    optiimizer: Optimizer to use for each epoch\n",
    "    loss_fn: Function to calculate loss\n",
    "    \"\"\"\n",
    "    train_losses = {}\n",
    "    \n",
    "    model.train()\n",
    "    print(\"=> Starting training\")\n",
    "    for epoch in range(epochs):\n",
    "        epoch_losses = list()\n",
    "        for X, Y in data:\n",
    "\n",
    "            # 1. data\n",
    "            hidden = model.init_zero_hidden(batch_size=X.shape[0])\n",
    "            X, Y, hidden = X.to(device), Y.to(device), hidden.to(device)\n",
    "\n",
    "            # 2. loss: one loss per sequence\n",
    "            loss = 0\n",
    "            for c in range(X.shape[1]):\n",
    "                out, hidden = model(X[:, c].reshape(X.shape[0],1), hidden)\n",
    "                l = loss_fn(out, Y[:, c].long())\n",
    "                loss += l\n",
    "\n",
    "            # 3. clear gradients\n",
    "            model.zero_grad()\n",
    "            \n",
    "            # 4. Compte gradients gradients\n",
    "            loss.backward()\n",
    "\n",
    "            # 5. Adjust learnable parameters\n",
    "            # clip as well to avoid vanishing and exploding gradients\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), 3)\n",
    "            optimizer.step()\n",
    "        \n",
    "            epoch_losses.append(loss.detach().item() / X.shape[1])\n",
    "\n",
    "        train_losses[epoch] = torch.tensor(epoch_losses).mean()\n",
    "        if (epoch % 100 == 0): print(f'=> epoch: {epoch + 1}, loss: {train_losses[epoch]}')\n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "length: 19916\n",
      "unique characters: ['\\n', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z']\n",
      "=> Starting training\n",
      "=> epoch: 1, loss: 2.9525957107543945\n",
      "=> epoch: 101, loss: 1.9783456325531006\n",
      "=> epoch: 201, loss: 1.7968411445617676\n",
      "=> epoch: 301, loss: 1.6473729610443115\n",
      "=> epoch: 401, loss: 1.5338435173034668\n",
      "=> epoch: 501, loss: 1.4334731101989746\n",
      "=> epoch: 601, loss: 1.3648561239242554\n",
      "=> epoch: 701, loss: 1.3004016876220703\n",
      "=> epoch: 801, loss: 1.2423133850097656\n",
      "=> epoch: 901, loss: 1.1996700763702393\n"
     ]
    }
   ],
   "source": [
    "if __name__ == \"__main__\":\n",
    "    # use any text file you want to learn\n",
    "    data = open('data/dinos.txt', 'r').read()\n",
    "    data = data.lower()\n",
    "\n",
    "    # Settings\n",
    "    seq_length = 25\n",
    "    batch_size = 64\n",
    "    hidden_size = 256\n",
    "\n",
    "    text_dataset = TextDataset(data, seq_length=seq_length)\n",
    "    text_dataloader = DataLoader(text_dataset, batch_size)\n",
    "    output_size = len(text_dataset.chars)\n",
    "\n",
    "    # test\n",
    "    print(\"length:\", len(data))\n",
    "    print(\"unique characters:\", text_dataset.chars)\n",
    "\n",
    "    # Model\n",
    "    rnnModel = RNN(1, hidden_size, output_size).to(device)\n",
    "\n",
    "    # Train variables\n",
    "    epochs = 1000\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    optimizer = optim.RMSprop(rnnModel.parameters(), lr = 0.001)\n",
    "\n",
    "    train(rnnModel, text_dataloader, epochs, optimizer, loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RNN(\n",
       "  (i2h): Linear(in_features=1, out_features=256, bias=False)\n",
       "  (h2h): Linear(in_features=256, out_features=256, bias=True)\n",
       "  (h2o): Linear(in_features=256, out_features=27, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 193,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# evaluation\n",
    "rnnModel.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starts with: p\n",
      "pngrga\n",
      "tur-\n"
     ]
    }
   ],
   "source": [
    "prediction_len = 10\n",
    "# first character\n",
    "hidden = rnnModel.init_zero_hidden()\n",
    "predicted = text_dataset.vector_to_string([random.randint(0, len(text_dataset.chars) -1)])\n",
    "print(\"starts with:\", predicted)\n",
    "\n",
    "# subsequent 9 characters\n",
    "for i in range(prediction_len - 1):\n",
    "    last_char = torch.Tensor([text_dataset.char_to_idx[predicted[-1]]])\n",
    "    X, hidden = last_char.to(device), hidden.to(device)\n",
    "    out, hidden = rnnModel(X, hidden)\n",
    "    result = torch.multinomial(nn.functional.softmax(out, 1), 1).item()\n",
    "    predicted += text_dataset.idx_to_char[result]\n",
    "predicted += \"-\"\n",
    "\n",
    "print(predicted)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

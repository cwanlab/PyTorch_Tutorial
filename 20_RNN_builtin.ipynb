{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torchvision # some builtin datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to one architechure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Settings\n",
    "\n",
    "# we will treat the image (28*28) as a sequence\n",
    "# look at one row at a time\n",
    "# It will be read 28 times: sequence_length\n",
    "batch_size = 100\n",
    "\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "\n",
    "num_classes = 10\n",
    "\n",
    "num_epoches = 2\n",
    "learning_rate = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zhenya/anaconda3/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "# MNIST\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", \n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),#Converts the images into PyTorch tensors. Each image is normalized to have values in the range [0,1]\n",
    "    download=True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", \n",
    "    train=False,\n",
    "    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "# [100, 1, 28, 28]: batch size 100, gray image with size 28 x28\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.rnn = nn.RNN(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # batch_first: If True, then the input and output tensors are provided as \n",
    "        # (batch, sequence_length, input_size)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.rnn(x, h0)\n",
    "        # batch_size, sequence_length, hidden size\n",
    "        out = out[:, -1, :] #only the last step\n",
    "        # batch_size, hidden size\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "\n",
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 1 / 2, step 100 / 600, loss =  0.8843\n",
      "epoch 1 / 2, step 200 / 600, loss =  0.6266\n",
      "epoch 1 / 2, step 300 / 600, loss =  0.5662\n",
      "epoch 1 / 2, step 400 / 600, loss =  0.4297\n",
      "epoch 1 / 2, step 500 / 600, loss =  0.3531\n",
      "epoch 1 / 2, step 600 / 600, loss =  0.3180\n",
      "epoch 2 / 2, step 100 / 600, loss =  0.3809\n",
      "epoch 2 / 2, step 200 / 600, loss =  0.1842\n",
      "epoch 2 / 2, step 300 / 600, loss =  0.3480\n",
      "epoch 2 / 2, step 400 / 600, loss =  0.3163\n",
      "epoch 2 / 2, step 500 / 600, loss =  0.2927\n",
      "epoch 2 / 2, step 600 / 600, loss =  0.1858\n",
      "acc = 93.5900\n"
     ]
    }
   ],
   "source": [
    "# Training loop\n",
    "num_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(images.shape) # torch.Size([100, 1, 28, 28]) --> 100, 784\n",
    "        # print(labels.shape) # torch.Size([100])\n",
    "        \n",
    "        # sequence_length = 28\n",
    "        # input_size = 28\n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epoches}, step {i+1} / {num_total_steps}, loss = {loss: 0.4f}')\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_eva = model(images)\n",
    "        _, y_eva_cls = torch.max(y_eva, 1)\n",
    "        \n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (y_eva_cls == labels).sum().item()\n",
    "    acc = 100*n_correct/n_samples\n",
    "    print(f'acc = {acc:.4f}')\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

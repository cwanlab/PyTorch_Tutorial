{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#From manually to pytorch\n",
    "# Prediction: manually --> xxx --> pytorch model\n",
    "# Loss computation: manually--> pytorch loss\n",
    "# Gradient computation: manually--> Autograd\n",
    "# Parameter updates: manually --> pytorch optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f = w * x\n",
    "# f = 2 * x\n",
    "\n",
    "X = np.array([1, 2, 3, 4], dtype=np.float32)\n",
    "y = np.array([2, 4, 6, 8], dtype=np.float32)\n",
    "\n",
    "W = 0.0\n",
    "\n",
    "# Model prediction\n",
    "def forward(X):\n",
    "    return W * X\n",
    "\n",
    "# loss: MSE\n",
    "def loss(y, y_predicted):\n",
    "    return ((y - y_predicted)**2).mean()\n",
    "\n",
    "# gradient\n",
    "# MSE = 1/N * (W*X - Y)**2\n",
    "# dJ/dw = 1/N 2X (XW - Y)\n",
    "def gradient(X, y, y_predicted):\n",
    "    return np.dot(2*X, y_predicted - y).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton before training: f(5) = 0.000\n",
      "epoch1: W =  1.200, loss =  30.00000000\n",
      "epoch2: W =  1.680, loss =  4.79999924\n",
      "epoch3: W =  1.872, loss =  0.76800019\n",
      "epoch4: W =  1.949, loss =  0.12288000\n",
      "epoch5: W =  1.980, loss =  0.01966083\n",
      "epoch6: W =  1.992, loss =  0.00314574\n",
      "epoch7: W =  1.997, loss =  0.00050331\n",
      "epoch8: W =  1.999, loss =  0.00008053\n",
      "epoch9: W =  1.999, loss =  0.00001288\n",
      "epoch10: W =  2.000, loss =  0.00000206\n",
      "Prediciton after training: f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediciton before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    dw = gradient(X, y, y_pred)\n",
    "\n",
    "    W -= learning_rate * dw\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch{epoch+1}: W = {W: 0.3f}, loss = {l: 0.8f}')\n",
    "\n",
    "print(f'Prediciton after training: f(5) = {forward(5):.3f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediciton before training: f(5) = 0.000\n",
      "epoch1: W =  1.200, loss =  30.00000000\n",
      "epoch2: W =  1.680, loss =  4.79999924\n",
      "epoch3: W =  1.872, loss =  0.76800019\n",
      "epoch4: W =  1.949, loss =  0.12288000\n",
      "epoch5: W =  1.980, loss =  0.01966083\n",
      "epoch6: W =  1.992, loss =  0.00314574\n",
      "epoch7: W =  1.997, loss =  0.00050331\n",
      "epoch8: W =  1.999, loss =  0.00008053\n",
      "epoch9: W =  1.999, loss =  0.00001288\n",
      "epoch10: W =  2.000, loss =  0.00000206\n",
      "Prediciton after training: f(5) = 9.999\n"
     ]
    }
   ],
   "source": [
    "print(f'Prediciton before training: f(5) = {forward(5):.3f}')\n",
    "\n",
    "# Training\n",
    "learning_rate = 0.01\n",
    "n_iters = 10\n",
    "\n",
    "for epoch in range(n_iters):\n",
    "    y_pred = forward(X)\n",
    "\n",
    "    l = loss(y, y_pred)\n",
    "\n",
    "    dw = gradient(X, y, y_pred)\n",
    "\n",
    "    W -= learning_rate * dw\n",
    "\n",
    "    if epoch % 1 == 0:\n",
    "        print(f'epoch{epoch+1}: W = {W: 0.3f}, loss = {l: 0.8f}')\n",
    "\n",
    "print(f'Prediciton after training: f(5) = {forward(5):.3f}')"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

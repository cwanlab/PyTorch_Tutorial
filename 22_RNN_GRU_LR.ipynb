{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim.lr_scheduler as lr_scheduler\n",
    "import torchvision # some builtin datasets\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import math\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Many to one architechure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Settings\n",
    "\n",
    "# device config\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"device:\", device)\n",
    "\n",
    "# hyper parameters\n",
    "\n",
    "# input_size = 28*28 \n",
    "# we will treat the image as a sequence\n",
    "# look at one row at a time\n",
    "input_size = 28\n",
    "sequence_length = 28\n",
    "\n",
    "hidden_size = 128\n",
    "num_layers = 2\n",
    "\n",
    "num_classes = 10\n",
    "num_epoches = 3\n",
    "batch_size = 100\n",
    "learning_rate = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# MNIST\n",
    "train_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", \n",
    "    train=True,\n",
    "    transform=transforms.ToTensor(),#Converts the images into PyTorch tensors. Each image is normalized to have values in the range [0,1]\n",
    "    download=True)\n",
    "\n",
    "test_data = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", \n",
    "    train=False,\n",
    "    transform=transforms.ToTensor())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n",
      "torch.Size([100, 1, 28, 28]) torch.Size([100])\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_data, \n",
    "    batch_size=batch_size,\n",
    "    shuffle=True)\n",
    "\n",
    "examples = iter(train_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "\n",
    "examples = iter(test_loader)\n",
    "samples, labels = examples.next()\n",
    "print(samples.shape, labels.shape)\n",
    "# [100, 1, 28, 28]: batch size 100, gray image with size 28 x28\n",
    "\n",
    "for i in range(6):\n",
    "    plt.subplot(2, 3, i+1)\n",
    "    plt.imshow(samples[i][0], cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class RNN(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes):\n",
    "        super(RNN, self).__init__()\n",
    "\n",
    "        self.num_layers = num_layers\n",
    "        self.hidden_size = hidden_size\n",
    "\n",
    "        self.gru = nn.GRU(input_size, hidden_size, num_layers, batch_first=True)\n",
    "        # batch_first: If True, then the input and output tensors are provided as (batch, seq, input_size)\n",
    "        \n",
    "        self.fc = nn.Linear(hidden_size, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_size).to(device)\n",
    "\n",
    "        out, _ = self.gru(x, h0)\n",
    "        # batch_size, sequence_length, hidden size\n",
    "        out = out[:, -1, :] #only the last step\n",
    "        # batch_size, hidden size\n",
    "\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]}\n",
      "{'state': {}, 'param_groups': [{'lr': 0.01, 'betas': (0.9, 0.999), 'eps': 1e-08, 'weight_decay': 0, 'amsgrad': False, 'initial_lr': 0.01, 'params': [0, 1, 2, 3, 4, 5, 6, 7, 8, 9]}]}\n",
      "epoch 1 / 3, step 100 / 600, loss =  0.3506\n",
      "epoch 1 / 3, step 200 / 600, loss =  0.1926\n",
      "epoch 1 / 3, step 300 / 600, loss =  0.1280\n",
      "epoch 1 / 3, step 400 / 600, loss =  0.1223\n",
      "epoch 1 / 3, step 500 / 600, loss =  0.2180\n",
      "epoch 1 / 3, step 600 / 600, loss =  0.0927\n",
      "learning rate: 0.001\n",
      "epoch 2 / 3, step 100 / 600, loss =  0.1328\n",
      "epoch 2 / 3, step 200 / 600, loss =  0.0085\n",
      "epoch 2 / 3, step 300 / 600, loss =  0.0652\n",
      "epoch 2 / 3, step 400 / 600, loss =  0.0162\n",
      "epoch 2 / 3, step 500 / 600, loss =  0.0217\n",
      "epoch 2 / 3, step 600 / 600, loss =  0.0458\n",
      "learning rate: 0.00010000000000000002\n",
      "epoch 3 / 3, step 100 / 600, loss =  0.0321\n",
      "epoch 3 / 3, step 200 / 600, loss =  0.0755\n",
      "epoch 3 / 3, step 300 / 600, loss =  0.0026\n",
      "epoch 3 / 3, step 400 / 600, loss =  0.0169\n",
      "epoch 3 / 3, step 500 / 600, loss =  0.0253\n",
      "epoch 3 / 3, step 600 / 600, loss =  0.0375\n",
      "learning rate: 1.0000000000000003e-05\n",
      "acc = 98.7300\n"
     ]
    }
   ],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "print(optimizer.state_dict())\n",
    "\n",
    "lambda1 = lambda epoch: 0.1**epoch\n",
    "scheduler = lr_scheduler.LambdaLR(optimizer, lambda1)\n",
    "print(optimizer.state_dict())\n",
    "\n",
    "# Training loop\n",
    "num_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(images.shape) # torch.Size([100, 1, 28, 28]) --> 100, 784\n",
    "        # print(labels.shape) # torch.Size([100])\n",
    "        \n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epoches}, step {i+1} / {num_total_steps}, loss = {loss: 0.4f}')\n",
    "    \n",
    "    scheduler.step()\n",
    "    print('learning rate:', optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_eva = model(images)\n",
    "        _, y_eva_cls = torch.max(y_eva, 1)\n",
    "        \n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (y_eva_cls == labels).sum().item()\n",
    "    acc = 100*n_correct/n_samples\n",
    "    print(f'acc = {acc:.4f}')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = RNN(input_size, hidden_size, num_layers, num_classes).to(device)\n",
    "\n",
    "# \n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "# 0.01\n",
    "# epoch == 1 --> 0.001\n",
    "# epoch == 2 --> 0.0001\n",
    "scheduler = lr_scheduler.StepLR(optimizer, step_size=1, gamma=0.1)\n",
    "print(optimizer.state_dict())\n",
    "\n",
    "# Training loop\n",
    "num_total_steps = len(train_loader)\n",
    "\n",
    "for epoch in range(num_epoches):\n",
    "    for i, (images, labels) in enumerate(train_loader):\n",
    "        # print(images.shape) # torch.Size([100, 1, 28, 28]) --> 100, 784\n",
    "        # print(labels.shape) # torch.Size([100])\n",
    "        \n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        \n",
    "        # forward\n",
    "        y_pred = model(images)\n",
    "        loss = criterion(y_pred, labels)\n",
    "        \n",
    "        # backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        if (i+1) % 100 == 0:\n",
    "            print(f'epoch {epoch+1} / {num_epoches}, step {i+1} / {num_total_steps}, loss = {loss: 0.4f}')\n",
    "    \n",
    "    scheduler.step()\n",
    "    print('learning rate:', optimizer.state_dict()['param_groups'][0]['lr'])\n",
    "\n",
    "# Evaluation\n",
    "with torch.no_grad():\n",
    "    n_correct = 0\n",
    "    n_samples = 0\n",
    "    for images, labels in test_loader:\n",
    "        images = images.view(batch_size, sequence_length, input_size).to(device)\n",
    "        labels = labels.to(device)\n",
    "        y_eva = model(images)\n",
    "        _, y_eva_cls = torch.max(y_eva, 1)\n",
    "        \n",
    "        n_samples += labels.shape[0]\n",
    "        n_correct += (y_eva_cls == labels).sum().item()\n",
    "    acc = 100*n_correct/n_samples\n",
    "    print(f'acc = {acc:.4f}')\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
